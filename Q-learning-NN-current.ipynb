{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and stuff\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.integrate as integrate\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import IPython\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Constants for discretizing states. There are 50 states of theta and thetadot each.\n",
    "State 0 corresponds to theta = -pi, thetadot = -6.5m/s\n",
    "State 2499 corresponds to theta = pi, thetadot = 6.5m/s\n",
    "States [24, 25, 2474 and 2475] have values that approximately equal theta = -pi , thetadot = 0 \n",
    "These 4 states are considered goal states.\n",
    "'''\n",
    "maxVelocity = 6.5\n",
    "thetaStates = 50\n",
    "thetaDotStates = 50\n",
    "thetaStepSize = 2*math.pi/49\n",
    "thetaDotStepSize = 2*maxVelocity/49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def pendulum(state,action):\n",
    "    \n",
    "    '''\n",
    "    This function takes state(number from 0 to 2499 ) and action (0, 1 or 2) and computes the next state\n",
    "    of the pendulum based on the action. \n",
    "    \n",
    "    '''\n",
    "    #constants\n",
    "    GRAVITY = 9.8\n",
    "    LENGTH = 1.0\n",
    "    dt1 = 0.001\n",
    "    FORCE = 4.0\n",
    "    time = 200\n",
    "    if (action == 0):\n",
    "        u = -FORCE\n",
    "    elif(action == 1):\n",
    "        u = FORCE\n",
    "    elif(action == 2):\n",
    "        u = 0\n",
    "    # Get values of theta and thetadot that correspond to the state\n",
    "    theta, thetaDot = get_values(state)\n",
    "    \n",
    "    # Do Euler update of values for theta and thetadot to calculate the new theta and thetadot \n",
    "    for i in range(time):\n",
    "        \n",
    "        newThetaDot = thetaDot + dt1*(-(GRAVITY/LENGTH)*np.sin(theta) + u)\n",
    "        newTheta = theta + dt1*newThetaDot\n",
    "        \n",
    "        theta = ((newTheta + np.pi)%(2*np.pi)) - np.pi\n",
    "        thetaDot = np.clip(newThetaDot, -maxVelocity, maxVelocity)\n",
    "    \n",
    "    #Discretize the values theta and thetadot back into a state(number from 0 to 2499)\n",
    "    next_state = get_state(newTheta, newThetaDot)\n",
    "    return next_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     11
    ]
   },
   "outputs": [],
   "source": [
    "def get_state(theta, thetaDot):\n",
    "    '''\n",
    "    Discretize the values theta and thetadot back into a state(number from 0 to 2499)\n",
    "    '''\n",
    "    state = 0\n",
    "    for i in range (thetaStates):\n",
    "        for j in range (thetaDotStates):\n",
    "            if ((theta >= (-np.pi + thetaStepSize*i)) & (thetaDot >=(-maxVelocity + thetaDotStepSize*j))):\n",
    "                state = i*thetaStates + j    \n",
    "    return state\n",
    "\n",
    "def get_values(state):\n",
    "    '''\n",
    "    Get values of theta and thetadot that correspond to the state\n",
    "    '''\n",
    "    thetaDot = -maxVelocity + (state%thetaDotStates)*thetaDotStepSize\n",
    "    theta = -np.pi + int(state/thetaStates)*thetaStepSize\n",
    "    return theta, thetaDot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def draw_pendulum(theta_array):\n",
    "    '''\n",
    "    This function takes an array of theta values for each time instant and creates an animation. \n",
    "    \n",
    "    '''\n",
    "    x1 = np.sin(theta_array)\n",
    "    y1 = -np.cos(theta_array)\n",
    "\n",
    "    x1= x1[:,0::200]\n",
    "    y1= y1[:,0::200]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, autoscale_on=False, xlim=(-2, 2), ylim=(-2, 2))\n",
    "    ax.grid()\n",
    "\n",
    "    line, = ax.plot([], [], 'o-', lw=2)\n",
    "    time_template = 'time = %.1fs'\n",
    "    time_text = ax.text(0.05, 0.9, '', transform=ax.transAxes)\n",
    "\n",
    "    dt = 0.0005\n",
    "    t = np.arange(0.0, 20, dt)\n",
    "\n",
    "    def init():\n",
    "        line.set_data([], [])\n",
    "        time_text.set_text('')\n",
    "        return line, time_text\n",
    "\n",
    "\n",
    "    def animate(i):\n",
    "        thisx = [0, x1[i]]\n",
    "        thisy = [0, y1[i]]\n",
    "\n",
    "        line.set_data(thisx, thisy)\n",
    "        time_text.set_text(time_template % (i*dt))\n",
    "        return line, time_text\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, animate, np.arange(1, len(theta_array)),\n",
    "                                  interval=200, blit=True, init_func=init)\n",
    "\n",
    "\n",
    "    plt.close(fig)\n",
    "    plt.close(ani._fig)\n",
    "    IPython.display.display_html(IPython.core.display.HTML(ani.to_html5_video()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-6-e16cb4e01766>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-e16cb4e01766>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    '''\u001b[0m\n\u001b[0m       \n^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# state transition table\n",
    "def generate_next_state():\n",
    "    \n",
    "'''\n",
    "This function makes a table of all possible states(rows) and \n",
    "actions(columns) and records next state according to the pendulum dynamics.  \n",
    "'''\n",
    "    next_states = np.zeros((thetaStates*thetaDotStates,3))\n",
    "    for i in range (thetaStates*thetaDotStates):\n",
    "        for j in range(3):\n",
    "            next_states[i,j] = int(pendulum(i,j))\n",
    "    #     print(next_states[i])\n",
    "    return next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# A matrix with next states, next_state = next_states[state,action]. This table can be used\n",
    "# to compute the next state of pendulum, given an action, at each step.\n",
    "next_states = generate_next_state()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "###  Defining Neural Network to store Q(s,a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H1, H2, D_out = 1, 2, 32, 32, 1\n",
    "\n",
    "# Create empty Tensor to hold input\n",
    "x = torch.empty(1, 2)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "policy_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H1),\n",
    "    torch.nn.Linear(H1, H2),\n",
    "    torch.nn.Linear(H2, D_out),)\n",
    "\n",
    "#target_net is used to freeze the target every few steps\n",
    "target_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H1),\n",
    "    torch.nn.Linear(H1, H2),\n",
    "    torch.nn.Linear(H2, D_out),)\n",
    "\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def run_episode():\n",
    "    #hyperparameters and initial conditions for each episode\n",
    "    currentState = 1225\n",
    "    currentAct = 1.0\n",
    "    episodeSteps = 500\n",
    "    EPSILON = 0.25\n",
    "    GAMMA = 0.999\n",
    "    TARGET_UPDATE = 10\n",
    "    Qa_values = np.zeros(3)\n",
    "    Qa1_values = np.zeros(3)\n",
    "    \n",
    "    for i in range(episodeSteps):\n",
    "        #get next state based on current state and current best action \n",
    "        nextState = next_states[int(currentState),int(currentAct)]\n",
    "        \n",
    "        # set the reward for current state\n",
    "        if((currentState ==24)|(currentState ==25)|(currentState ==2474)|(currentState ==2475)):\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = -1\n",
    "        \n",
    "        #######\n",
    "        #evaluate Q(s,a) at current state and current best action from neural net\n",
    "        x[0][0] = currentState\n",
    "        x[0][1] = float(currentAct)\n",
    "        state_action_values = policy_net(x)\n",
    "        state_action_values.requires_grad_(True)\n",
    "        #######\n",
    "        \n",
    "        #######\n",
    "        #evaluate max_a(Q(s',a)) at current state and current best action from 'target' neural net \n",
    "        x[0][0] = nextState\n",
    "        x[0][1] = 0\n",
    "        Qa1_values[0] = target_net(x).detach()\n",
    "        x[0][1] = 1\n",
    "        Qa1_values[1] = target_net(x).detach()\n",
    "        x[0][1] = 2\n",
    "        Qa1_values[2] = target_net(x).detach()\n",
    "        nextAct1 = np.argmax(Qa1_values)\n",
    "        x[0][1] = float(nextAct1)\n",
    "        next_state_values = target_net(x).detach()\n",
    "        #######\n",
    "        \n",
    "        # Target value for our loss function\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward\n",
    "        expected_state_action_values.requires_grad_(True)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(state_action_values, expected_state_action_values)\n",
    "        loss = Variable(loss, requires_grad = True)\n",
    "        \n",
    "        # Update policy\n",
    "        policy_net.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        ########\n",
    "        # Best next action for next state, evaluated from our model, policy_net. \n",
    "        if (EPSILON > random.randint(1,10)*0.1):\n",
    "            nextAct = np.array(random.randint(0,2))\n",
    "        else:\n",
    "            x[0][0] = nextState\n",
    "            \n",
    "            x[0][1] = 0\n",
    "            Qa_values[0] = policy_net(x).detach()\n",
    "\n",
    "\n",
    "            x[0][1] = 1\n",
    "            Qa_values[1] = policy_net(x).detach()\n",
    "\n",
    "\n",
    "            x[0][1] = 2\n",
    "            Qa_values[2] = policy_net(x).detach()\n",
    "\n",
    "            nextAct = np.argmax(Qa_values)\n",
    "        ########\n",
    "        \n",
    "        \n",
    "        #update state\n",
    "        currentState = nextState\n",
    "        currentAct = nextAct\n",
    "        # Update the target network, copying all weights and biases in policy_net\n",
    "        if i % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run episodes  \n",
    "for i in range(1000):\n",
    "    run_episode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I'm trying to get a sequence of actions from the model that has already learned.\n",
    "# Start at the pendulum down position at state = 1225\n",
    "St = 1224\n",
    "NextValue = np.zeros((3,1))\n",
    "policy = np.zeros((500,1))\n",
    "\n",
    "# Get 500 steps from the trained model\n",
    "# Get the best next step for each state by comparing all three state-action values.\n",
    "# Store each step in policy \n",
    "for i in range(500):\n",
    "    policy[i] = St\n",
    "\n",
    "    x[0][0] = St\n",
    "    x[0][1] = 0\n",
    "    NextValue[0] = target_net(x).detach()\n",
    "    print('val act 0',NextValue[0])\n",
    "    \n",
    "    x[0][1] = 1\n",
    "    NextValue[1] = target_net(x).detach()\n",
    "    print('val act 1',NextValue[1])\n",
    "    \n",
    "    x[0][1] = 2\n",
    "    NextValue[2] = target_net(x).detach()\n",
    "    print('val act 2',NextValue[2])\n",
    "    \n",
    "    nextAct = int(np.argmax(NextValue))\n",
    "    print(nextAct)\n",
    "    newState = next_states[St,nextAct]\n",
    "    St = int(newState)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the policy\n",
    "thetaVals = np.zeros((100,1))\n",
    "for i in range(100):\n",
    "    \n",
    "    st = policy[i]\n",
    "    thetaVals[i],_ = get_values(st)\n",
    "    \n",
    "    \n",
    "plt.plot(range(100), thetaVals)\n",
    "draw_pendulum(thetaVals)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"theta$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def freependulum(state):\n",
    "    '''\n",
    "    To check dynamics of pendulum without control. \n",
    "    This function takes a state value from 0 to 2499 and displays SHM of pendulum with 0 force.  \n",
    "    '''\n",
    "    freeswing = np.zeros([100,1])\n",
    "    freeswingtheta = np.zeros([100,1])\n",
    "    for i in range(50):\n",
    "        freeswing[i] = next_states[state,2]\n",
    "        sat = int(freeswing[i])\n",
    "        freeswingtheta[i],_ = get_values(state)\n",
    "\n",
    "\n",
    "    plt.plot(range(100), freeswingtheta)\n",
    "    draw_pendulum(freeswingtheta)\n",
    "    plt.xlabel(\"time (ms)\")\n",
    "    plt.ylabel(\"theta\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freependulum(1035)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
